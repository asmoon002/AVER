{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, dim, num_layers, dropout, residual_embeddings=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.rnn_dim = dim // 2\n",
    "        self.linear = nn.Linear(dim + embed_size, dim)\n",
    "        self.rnn = nn.LSTM(embed_size, self.rnn_dim, num_layers=num_layers, dropout=dropout,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        self.residual_embeddings = residual_embeddings\n",
    "        self.init_hidden = nn.Parameter(nn.init.xavier_uniform_(torch.zeros(2 * 2 * num_layers, self.rnn_dim)))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch = inputs.size(0)\n",
    "        h0 = self.init_hidden[:2 * self.num_layers].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "        c0 = self.init_hidden[2 * self.num_layers:].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "\n",
    "        print(\"LSTM inputs : \", inputs.shape)\n",
    "        outputs, hidden_t = self.rnn(inputs, (h0, c0))\n",
    "\n",
    "        if self.residual_embeddings:\n",
    "            outputs = torch.cat([inputs, outputs], dim=-1)\n",
    "        outputs = self.linear(self.dropout(outputs))\n",
    "\n",
    "        return F.normalize(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "class DenseCoAttn(nn.Module):\n",
    "\tdef __init__(self, dim1, dim2, dropout): #dim1, dim2 = 512, 512\n",
    "\t\tsuper(DenseCoAttn, self).__init__()\n",
    "\t\tdim = dim1 + dim2\n",
    "\t\tself.dropouts = nn.ModuleList([nn.Dropout(p=dropout) for _ in range(2)])\n",
    "\t\tself.query_linear = nn.Linear(dim, dim)\n",
    "\t\tself.key1_linear = nn.Linear(16, 16)\n",
    "\t\tself.key2_linear = nn.Linear(16, 16)\n",
    "\t\tself.value1_linear = nn.Linear(dim1, dim1)\n",
    "\t\tself.value2_linear = nn.Linear(dim2, dim2)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, value1, value2):\n",
    "\t\tprint(\"DenseCoAttn input value1(video) : \", value1.shape) # 16, 16, 512\n",
    "\t\tprint(\"DenseCoAttn input value2(audio) : \", value2.shape)\n",
    "\t\tjoint = torch.cat((value1, value2), dim=-1)\n",
    "\t\t# audio  audio*W*joint\n",
    "\t\tjoint = self.query_linear(joint)\n",
    "\t\tprint(\"DenseCoAttn joint representation : \", joint.shape)\n",
    "\t\tkey1 = self.key1_linear(value1.transpose(1, 2)) # X_v^T\n",
    "\t\tkey2 = self.key2_linear(value2.transpose(1, 2)) # X_a^T \n",
    "\t\tprint(\"DenseCoAttn X_v^T : \", key1.shape) # 16, 512, 16\n",
    "\t\tprint(\"DenseCoAttn X_a^T : \", key2.shape)\n",
    "\n",
    "\t\tvalue1 = self.value1_linear(value1) # 16, 16, 512 (Can't understanding Layer)\n",
    "\t\tvalue2 = self.value2_linear(value2) # (Can't understanding Layer)\n",
    "\t\tprint(\"DenseCoAttn value1 after value_linear : \", value1.shape)\n",
    "\t\tprint(\"DenseCoAttn value2 after value_linear : \", value2.shape)\n",
    "\n",
    "\t\tweighted1, attn1 = self.qkv_attention(joint, key1, value1, dropout=self.dropouts[0])\n",
    "\t\tweighted2, attn2 = self.qkv_attention(joint, key2, value2, dropout=self.dropouts[1])\n",
    "\t\tprint(\"DenseCoAttn weighted1 : \", weighted1.shape)\n",
    "\t\tprint(\"DenseCoAttn weighted2 : \", weighted2.shape)\n",
    "\n",
    "\t\treturn weighted1, weighted2\n",
    "\n",
    "\tdef qkv_attention(self, query, key, value, dropout=None):\n",
    "\t\td_k = query.size(-1)\n",
    "\t\tscores = torch.bmm(key, query) / math.sqrt(d_k)\n",
    "\t\tscores = torch.tanh(scores) # C_v, C_a\n",
    "\t\tif dropout:\n",
    "\t\t\tscores = dropout(scores)\n",
    "\n",
    "\t\tweighted = torch.tanh(torch.bmm(value, scores))\n",
    "\t\treturn self.relu(weighted), scores # self.relu(weighted) == H_v, H_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout): # dim1, dim2 = 512, 512\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.dense_coattn = DenseCoAttn(dim1, dim2, dropout)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim1), # 1024, 512\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Dropout(p=dropout),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim2),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        weighted1, weighted2 = self.dense_coattn(data1, data2) # weighted1, weighted2 = 1024, 1024\n",
    "        data1 = data1 + self.linears[0](weighted1) # X_att,v^t\n",
    "        data2 = data2 + self.linears[1](weighted2) # X_att,a^t\n",
    "\n",
    "        print(\"DCNLayer X_att,v : \" , data1.shape)\n",
    "        print(\"DCNLayer X_att,a : \" , data2.shape)\n",
    "\n",
    "        return data1, data2\n",
    "\n",
    "\n",
    "class DCNLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, num_seq, dropout): # dim1, dim2 = 512, 512\n",
    "        super(DCNLayer, self).__init__()\n",
    "        self.dcn_layers = nn.ModuleList([NormalSubLayer(dim1, dim2, dropout) for _ in range(num_seq)]) # 여기서 t-th iteration만큼 계산\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        for dense_coattn in self.dcn_layers:\n",
    "            data1, data2 = dense_coattn(data1, data2)\n",
    "\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "class BottomUpExtract(nn.Module):\n",
    "\tdef __init__(self, emed_dim, dim):\n",
    "\t\tsuper(BottomUpExtract, self).__init__()\n",
    "\t\tself.attn = PositionAttn(emed_dim, dim)\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tfeat = self.attn(video, audio)\n",
    "\n",
    "\t\treturn feat\n",
    "\n",
    "# audio-guided attention\n",
    "class PositionAttn(nn.Module):\n",
    "\n",
    "\tdef __init__(self, embed_dim, dim):\n",
    "\t\tsuper(PositionAttn, self).__init__()\n",
    "\t\tself.affine_audio = nn.Linear(embed_dim, dim)\n",
    "\t\tself.affine_video = nn.Linear(512, dim)\n",
    "\t\tself.affine_v = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_g = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_h = nn.Linear(49, 1, bias=False)\n",
    "\t\tself.affine_feat = nn.Linear(512, dim)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tv_t = video.view(video.size(0) * video.size(1), -1, 512).contiguous()\n",
    "\t\tV = v_t\n",
    "\n",
    "\t\t# Audio-guided visual attention\n",
    "\t\tv_t = self.relu(self.affine_video(v_t))\n",
    "\t\ta_t = audio.view(-1, audio.size(-1))\n",
    "\n",
    "\t\ta_t = self.relu(self.affine_audio(a_t))\n",
    "\n",
    "\t\tcontent_v = self.affine_v(v_t) \\\n",
    "\t\t\t\t\t+ self.affine_g(a_t).unsqueeze(2)\n",
    "\n",
    "\t\tz_t = self.affine_h((torch.tanh(content_v))).squeeze(2)\n",
    "\n",
    "\t\talpha_t = F.softmax(z_t, dim=-1).view(z_t.size(0), -1, z_t.size(1))  # attention map\n",
    "\n",
    "\t\tc_t = torch.bmm(alpha_t, V).view(-1, 512)\n",
    "\t\tvideo_t = c_t.view(video.size(0), -1, 512)\n",
    "\n",
    "\t\tvideo_t = self.affine_feat(video_t)\n",
    "\n",
    "\t\treturn video_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Chomp1d removes extra padding after Conv1d\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size]\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"Single Temporal Block\"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"Stacked Temporal Blocks\"\"\"\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TLAB(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(TLAB, self).__init__()\n",
    "        self.lstm = LSTM(input_dim, hidden_dim, num_layers=2, dropout=0.1, residual_embeddings=True)\n",
    "        self.tcn = TemporalConvNet(\n",
    "            num_inputs=input_dim, num_channels=[hidden_dim, hidden_dim], kernel_size=3, dropout=0.1\n",
    "        )\n",
    "        # Attention Mechanism\n",
    "        self.query_fc = nn.Linear(hidden_dim, hidden_dim)  # Q (from LSTM output)\n",
    "        self.key_fc = nn.Linear(hidden_dim, hidden_dim)    # K (from TCN output)\n",
    "        self.value_fc = nn.Linear(hidden_dim, hidden_dim)  # V (from TCN output)\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)        # Softmax for Attention weights\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract global and local features\n",
    "        lstm_feat = self.lstm(x)  # Output: (batch, seq_len, hidden_dim)\n",
    "        print(\"lstm_feat Output : \", lstm_feat.shape)\n",
    "        tcn_feat = self.tcn(x.transpose(1, 2)).transpose(1, 2)  # Output: (batch, seq_len, hidden_dim)\n",
    "        print(\"tcn_feat Output : \", tcn_feat.shape)\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.query_fc(lstm_feat)  # (batch, seq_len, hidden_dim)\n",
    "        K = self.key_fc(tcn_feat)    # (batch, seq_len, hidden_dim)\n",
    "        V = self.value_fc(tcn_feat)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Compute Attention weights\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-1, -2))  # (batch, seq_len, seq_len)\n",
    "        attention_weights = self.attention_softmax(attention_scores)  # Normalize scores\n",
    "\n",
    "        # Weighted sum of V\n",
    "        attended_feat = torch.matmul(attention_weights, V)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Combine attended features with global (LSTM) features\n",
    "        tlab_output_feat = lstm_feat + attended_feat  # (batch, seq_len, hidden_dim)\n",
    "        print(\"TLAB output shape : \", tlab_output_feat.shape)\n",
    "\n",
    "        return tlab_output_feat\n",
    "\n",
    "\n",
    "class LSTM_CAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_CAM, self).__init__()\n",
    "        self.coattn = DCNLayer(512, 512, 1, 0.6)\n",
    "        self.avga = BottomUpExtract(512, 512)\n",
    "\n",
    "        # Audio and Video TLABs\n",
    "        self.audio_tlab = TLAB(512, 512)\n",
    "        self.video_tlab = TLAB(512, 512)\n",
    "\n",
    "\n",
    "        # self.audio_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True) # output: (batch, sequence, features)\n",
    "        # self.video_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True) # output: (batch, sequence, features)\n",
    "\n",
    "        self.vregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        self.Joint = LSTM(1024, 512, 2, dropout=0, residual_embeddings=True)\n",
    "\n",
    "        self.aregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(net, init_type='xavier', init_gain=1):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "\n",
    "        def init_func(m):  # define the initialization function\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    init.uniform_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    init.xavier_uniform_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        print('initialize network with %s' % init_type)\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "    def forward(self, f1_norm, f2_norm):\n",
    "        video = F.normalize(f2_norm, dim=-1)\n",
    "        audio = F.normalize(f1_norm, dim=-1)\n",
    "\n",
    "        # video = self.avga(video, audio)\n",
    "        \n",
    "        # # Tried with LSTMs also\n",
    "        # audio_tcn = self.audio_tcn(audio)\n",
    "        # audio_lstm = self.audio_extract(audio)\n",
    "\n",
    "        audio = self.audio_tlab(audio)\n",
    "        print(\"audio_tlab feat : \", audio.shape)\n",
    "\n",
    "        video = self.avga(video, audio)\n",
    "        \n",
    "        video = self.video_tlab(video)\n",
    "        print(\"video_tlab feat : \", video.shape)\n",
    "        # video_tcn = self.video_tcn(video)\n",
    "        # video_lstm = self.video_extract(video)\n",
    "\n",
    "        video, audio = self.coattn(video, audio)\n",
    "\n",
    "        audiovisualfeatures = torch.cat((video, audio), -1)\n",
    "        \n",
    "        audiovisualfeatures = self.Joint(audiovisualfeatures)\n",
    "        vouts = self.vregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "        aouts = self.aregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "\n",
    "        return vouts.squeeze(2), aouts.squeeze(2)  #final_aud_feat.transpose(1,2), final_vis_feat.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.utils as utils\n",
    "from EvaluationMetrics.cccmetric import ccc\n",
    "\n",
    "import logging\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "learning_rate_decay_start = 5  # 50\n",
    "learning_rate_decay_every = 2 # 5\n",
    "learning_rate_decay_rate = 0.8 # 0.9\n",
    "total_epoch = 30\n",
    "lr = 0.0001\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch, lr, cam, time_chk_path):\n",
    "\tprint('\\nEpoch: %d' % epoch)\n",
    "\tglobal Train_acc\n",
    "\t#wandb.watch(audiovisual_model, log_freq=100)\n",
    "\t#wandb.watch(cam, log_freq=100)\n",
    "\n",
    "\t# switch to train mode\n",
    "\t#audiovisual_model.train()\n",
    "\tmodel.eval()\n",
    "\tcam.train()\n",
    "\n",
    "\tepoch_loss = 0\n",
    "\tvout = list()\n",
    "\tvtar = list()\n",
    "\n",
    "\taout = list()\n",
    "\tatar = list()\n",
    "\n",
    "\tif epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "\t\tfrac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "\t\tdecay_factor = learning_rate_decay_rate ** frac\n",
    "\t\tcurrent_lr = lr * decay_factor\n",
    "\t\tutils.set_lr(optimizer, current_lr)  # set the decayed rate\n",
    "\telse:\n",
    "\t\tcurrent_lr = lr\n",
    "\t######## chckpoint 없을 때 이거부터 ##########\n",
    "\tutils.set_lr(optimizer, current_lr)\n",
    "\t############################################\n",
    "\tprint('learning_rate: %s' % str(current_lr))\n",
    "\tlogging.info(\"Learning rate\")\n",
    "\tlogging.info(current_lr)\n",
    "\t#torch.cuda.synchronize()\n",
    "\t#t1 = time.time()\n",
    "\tn = 0\n",
    "\tif time_chk_path:\n",
    "\t\ttime_chk_file = os.path.join(time_chk_path, \"time_chk.txt\")\n",
    "\n",
    "\tglobal_vid_fts, global_aud_fts= None, None\n",
    "  \n",
    "\n",
    "\tfor batch_idx, (visualdata, audiodata, labels_V, labels_A) in tqdm(enumerate(train_loader),\n",
    "\t\t\t\t \t\t\t\t\t\t\t\t\t\t total=len(train_loader), position=0, leave=True):\n",
    "     \n",
    "\t\tprint(\"====\" * 20)\n",
    "\t\tprint(\"Batch Index : \", batch_idx)\n",
    "\n",
    "\t\tprint(\"====\" * 20)\n",
    "\t\tprint(\"visualdata : \", visualdata.shape)\n",
    "\t\tprint(\"audiodata : \", audiodata.shape)\n",
    "\t\tprint(\"labels_V : \", labels_V.shape)\n",
    "\t\tprint(\"labels_A : \", labels_A.shape)\n",
    "\n",
    "\t\tprint(\"====\" * 20)\n",
    "\n",
    "\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\taudiodata = audiodata.cuda()#.unsqueeze(2)\n",
    "\n",
    "\t\tvisualdata = visualdata.cuda()#permute(0,4,1,2,3).cuda()\n",
    "  \n",
    "\t\tst2 = time.time()\n",
    "\n",
    "\n",
    "\t\twith torch.cuda.amp.autocast():\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tb, seq_t, c, subseq_t, h, w = visualdata.size()\n",
    "\t\t\t\tvisual_feats = torch.empty((b, seq_t, 25088), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\t\t\t\taud_feats = torch.empty((b, seq_t, 512), dtype=visualdata.dtype, device = visualdata.device)\n",
    "\n",
    "\t\t\t\tfor i in range(visualdata.shape[0]):\n",
    "\t\t\t\t\tst1 = time.time()\n",
    "\t\t\t\t\taud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "\t\t\t\t\ted1 = time.time()\n",
    "\n",
    "\t\t\t\t\tpre_trained_model_time = ed1 - st1\n",
    "\t\t\t\t\tif time_chk_path:\n",
    "\t\t\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\t\t\tf.write(f\"Time pre_trained_model: {pre_trained_model_time}\\n\")\n",
    "\t\t\t\t\t# visual_feats[i,:,:] = visualfeat\n",
    "\t\t\t\t\tvisual_feats[i,:,:] = visualfeat.view(seq_t, -1)\n",
    "\t\t\t\t\taud_feats[i,:,:] = aud_feat\n",
    "\n",
    "\t\t\tst2 = time.time()\n",
    "\t\t\t# if batch_idx==0:\n",
    "\t\t\t\t# audiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats)\n",
    "\t\t\t# else:\n",
    "\t\t\t\t# audiovisual_vouts,audiovisual_aouts, global_vid_fts, global_aud_fts = cam(aud_feats, visual_feats, global_vid_fts, global_aud_fts)\n",
    "\t\n",
    "\t\t\taudiovisual_vouts,audiovisual_aouts = cam(aud_feats, visual_feats)\n",
    "\t\t\tprint(\"audiovisual_vouts : \" , audiovisual_vouts.shape)\n",
    "\t\t\tprint(\"audiovisual_aouts : \" , audiovisual_aouts.shape)\n",
    "\t\t\ted2 = time.time()\n",
    "   \n",
    "\t\t\ttime_cam_model= ed2 - st2\n",
    "\t\t\tif time_chk_path:\n",
    "\t\t\t\twith open(time_chk_file, 'a') as f:\n",
    "\t\t\t\t\tf.write(f\"Time cam model: {time_cam_model}\\n\")\n",
    "\t\t\t\t\tf.write(f\"Epoch: {epoch}\\n\")\n",
    "\t\t\t\t\tf.write(f\"batch_idx: {batch_idx}\\n\")\n",
    "\t\t\t\t\tf.write(\"----\"*20)\n",
    "\t\t\t\t\tf.write(\"\\n\")\n",
    "\t\t\t\tf.close()\n",
    "\n",
    "\t\t\tvoutputs = audiovisual_vouts.view(-1, audiovisual_vouts.shape[0]*audiovisual_vouts.shape[1])\n",
    "\t\t\taoutputs = audiovisual_aouts.view(-1, audiovisual_aouts.shape[0]*audiovisual_aouts.shape[1])\n",
    "\t\t\tvtargets = labels_V.view(-1, labels_V.shape[0]*labels_V.shape[1]).cuda()\n",
    "\t\t\tatargets = labels_A.view(-1, labels_A.shape[0]*labels_A.shape[1]).cuda()\n",
    "   \n",
    "\t\t\tv_loss = criterion(voutputs, vtargets)\n",
    "\t\t\ta_loss = criterion(aoutputs, atargets)\n",
    "   \n",
    "\t\t\tfinal_loss = v_loss + a_loss\n",
    "   \n",
    "\t\t\tepoch_loss += final_loss.cpu().data.numpy()\n",
    "\n",
    "\t\t# scaler.scale(final_loss).backward(retain_graph=True)\n",
    "\t\t# scaler.step(optimizer)\n",
    "\t\t# scaler.update()\n",
    "\n",
    "\t\twith torch.autograd.set_detect_anomaly(True):\n",
    "\t\t\tfinal_loss.backward(retain_graph=True)\n",
    "\t\t\toptimizer.step()\n",
    "\t\tn = n + 1\n",
    "\n",
    "\t\tvout = vout + voutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tvtar = vtar + vtargets.squeeze(0).detach().cpu().tolist()\n",
    "\n",
    "\t\taout = aout + aoutputs.squeeze(0).detach().cpu().tolist()\n",
    "\t\tatar = atar + atargets.squeeze(0).detach().cpu().tolist()\n",
    "\n",
    "\t\tbreak\n",
    "  \n",
    "\tscheduler.step(epoch_loss / n)\n",
    "\n",
    "\tif (len(vtar) > 1):\n",
    "\t\ttrain_vacc = ccc(vout, vtar)\n",
    "\t\ttrain_aacc = ccc(aout, atar)\n",
    "\telse:\n",
    "\t\ttrain_acc = 0\n",
    "\tprint(\"Train Accuracy\")\n",
    "\tprint(train_vacc)\n",
    "\tprint(train_aacc)\n",
    " \n",
    "\treturn train_vacc, train_aacc, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with xavier\n",
      "Fusion Model :  LSTM_CAM\n",
      "==> Preparing data..\n",
      "Train Data\n",
      "Number of Sequences: 247\n",
      "Number of Train samples:68709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "from datasets.dataset_new import ImageList\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from losses.loss import CCCLoss\n",
    "from datetime import datetime, timedelta\n",
    "from torch import nn\n",
    "import json\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "is_time_chk = False\n",
    "\n",
    "best_Val_acc = 0  # best PrivateTest accuracy\n",
    "#best_Val_acc = 0  # best PrivateTest accuracy\n",
    "best_Val_acc_epoch = 0\n",
    "\n",
    "TrainingAccuracy_V = []\n",
    "TrainingAccuracy_A = []\n",
    "ValidationAccuracy_V = []\n",
    "ValidationAccuracy_A = []\n",
    "\n",
    "Logfile_name = \"LogFiles/\" + \"log_file.log\"\n",
    "logging.basicConfig(filename=Logfile_name, level=logging.INFO)\n",
    "\n",
    "SEED = int(0)\n",
    "    \n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "class TrainPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tlabelV = [x[2] for x in sorted_batch]\n",
    "\t\tlabelA = [x[3] for x in sorted_batch]\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\n",
    "\t\treturn visual_sequences, audio_features, labelsV, labelsA\n",
    "\n",
    "\n",
    "class ValPadSequence:\n",
    "\tdef __call__(self, sorted_batch):\n",
    "\n",
    "\t\tsequences = [x[0] for x in sorted_batch]\n",
    "\t\taud_sequences = [x[1] for x in sorted_batch]\n",
    "\t\tspec_dim = []\n",
    "\t\tfor aud in aud_sequences:\n",
    "\t\t\tspec_dim.append(aud.shape[3])\n",
    "\n",
    "\t\tmax_spec_dim = max(spec_dim)\n",
    "\t\taudio_features = torch.zeros(len(spec_dim), 16, 1, 64, max_spec_dim)\n",
    "\t\tfor batch_idx, spectrogram in enumerate(aud_sequences):\n",
    "\t\t\tif spectrogram.shape[2] < max_spec_dim:\n",
    "\t\t\t\taudio_features[batch_idx, :, :, :, -spectrogram.shape[3]:] = spectrogram\n",
    "\t\t\telse:\n",
    "\t\t\t\taudio_features[batch_idx, :,:, :, :] = spectrogram\n",
    "\n",
    "\t\tframeids = [x[2] for x in sorted_batch]\n",
    "\t\tv_ids = [x[3] for x in sorted_batch]\n",
    "\t\tv_lengths = [x[4] for x in sorted_batch]\n",
    "\t\tlabelV = [x[5] for x in sorted_batch]\n",
    "\t\tlabelA = [x[6] for x in sorted_batch]\n",
    "\n",
    "\t\tvisual_sequences = torch.stack(sequences)\n",
    "\t\tlabelsV = torch.stack(labelV)\n",
    "\t\tlabelsA = torch.stack(labelA)\n",
    "\t\treturn visual_sequences, audio_features, frameids, v_ids, v_lengths, labelsV, labelsA\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"SavedWeights\"):\n",
    "\tos.makedirs(\"SavedWeights\", exist_ok=True)\n",
    "\n",
    "weight_save_path = \"SavedWeights\"\n",
    "\n",
    "result_save_path =\"save\"\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)\n",
    "\n",
    "### Loading audiovisual model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "### Freezing the model\n",
    "for p in model.parameters():\n",
    "\tp.requires_grad = False\n",
    "for p in model.children():\n",
    "\tp.train(False)\n",
    " \n",
    "fusion_model = LSTM_CAM()\n",
    "\n",
    "print_model_name = fusion_model.__class__.__name__\n",
    "print(\"Fusion Model : \", print_model_name)\n",
    "\n",
    "fusion_model = fusion_model.to(device=device)\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "def matching_files(root_path, anno_path):\n",
    "\tanno_list = []\n",
    "\tfor f in os.listdir(anno_path):\n",
    "\t\tanno_list.append(f.split(\".\")[0])\n",
    "  \n",
    "\troot_path_list = os.listdir(root_path)\n",
    "\t\n",
    "\tfor f in os.listdir(root_path):\n",
    "\t\tif not f in anno_list:\n",
    "\t\t\tdel root_path_list[root_path_list.index(f)]\n",
    "\n",
    "\treturn root_path_list\n",
    "\n",
    "def train_val_test_split(root_path, anno_path, seed=0):\n",
    "\trandom.seed(seed)\n",
    "\ttrial_data = matching_files(root_path, anno_path)\n",
    " \n",
    "\tfname_dict = {i:f for i,f in enumerate(trial_data)}\n",
    "\tlength = len(fname_dict)\n",
    " \n",
    "\tprint(\"full trial length: \", len(fname_dict))\n",
    "\n",
    "\ttrain_set = []\n",
    "\tvalid_set = []\n",
    "\ttest_set = []\n",
    " \n",
    "\ttrain_list_idx = random.sample(fname_dict.keys(), int(length*0.6))\n",
    "\tfor i in train_list_idx:\n",
    "\t\ttrain_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\t\t\n",
    "\tvalid_list_idx = random.sample(fname_dict.keys(), int(length*0.2))\n",
    "\tfor i in valid_list_idx:\n",
    "\t\tvalid_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "\n",
    "\ttest_list_idx = random.sample(fname_dict.keys(), int(length*0.2))    \n",
    "\tfor i in test_list_idx:\n",
    "\t\ttest_set.append(fname_dict[i]+\".csv\")\n",
    "\t\tdel fname_dict[i]\n",
    "  \n",
    "\treturn train_set, valid_set, test_set\n",
    "    \n",
    "with open('config_file.json', 'r') as f:\n",
    "\tconfiguration = json.load(f)\n",
    "\n",
    "dataset_rootpath = configuration['dataset_rootpath']\n",
    "dataset_wavspath = configuration['dataset_wavspath']\n",
    "dataset_labelpath = configuration['labelpath']\n",
    "\n",
    "def load_partition_set(partition_path, seed):\n",
    "\timport json\n",
    "\n",
    "\twith open(partition_path, 'r') as f:    \n",
    "\t\tseed_data = json.load(f)\n",
    "\n",
    "\tseed_data_train = seed_data[f'seed_{seed}']['Train_Set']\n",
    "\tseed_data_valid = seed_data[f'seed_{seed}']['Validation_Set']\n",
    "\tseed_data_test  = seed_data[f'seed_{seed}']['Test_Set']\n",
    " \n",
    "\tseed_data_train = [fn + \".csv\" for fn in seed_data_train]\n",
    "\tseed_data_valid = [fn + \".csv\" for fn in seed_data_valid]\n",
    "\tseed_data_test  = [fn + \".csv\" for fn in seed_data_test ]\n",
    "\n",
    "\treturn seed_data_train, seed_data_valid, seed_data_test\n",
    "\n",
    "partition_path = \"../data/Affwild2/seed_data.json\"\n",
    " \n",
    "train_set, valid_set, test_set = load_partition_set(partition_path, SEED)\n",
    "\n",
    "init_time = datetime.now()\n",
    "init_time = init_time.strftime('%m%d_%H%M')\n",
    "\n",
    "root_time_chk_dir = \"time_chk\"\n",
    "\n",
    "time_chk_path = None\n",
    "\n",
    "print(\"Train Data\")\n",
    "traindataset = ImageList(root=configuration['dataset_rootpath'], fileList=train_set, labelPath=dataset_labelpath,\n",
    "                        audList=configuration['dataset_wavspath'], length=configuration['train_params']['seq_length'],\n",
    "                        flag='train', stride=configuration['train_params']['stride'], dilation = configuration['train_params']['dilation'],\n",
    "                        subseq_length = configuration['train_params']['subseq_length'], time_chk_path=time_chk_path)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                traindataset, collate_fn=TrainPadSequence(),\n",
    "                **configuration['train_params']['loader_params'])\n",
    "print(\"Number of Train samples:\" + str(len(traindataset)))\n",
    "\n",
    "criterion = CCCLoss(digitize_num=1).cuda()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(),# filter(lambda p: p.requires_grad, multimedia_model.parameters()),\n",
    "\t\t\t\t\t\t\t\tconfiguration['model_params']['lr'])\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "cnt = 0\n",
    "fusion_model_name = 'gat'\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Batch Index :  0\n",
      "================================================================================\n",
      "visualdata :  torch.Size([16, 16, 3, 8, 112, 112])\n",
      "audiodata :  torch.Size([16, 16, 1, 64, 104])\n",
      "labels_V :  torch.Size([16, 16])\n",
      "labels_A :  torch.Size([16, 16])\n",
      "================================================================================\n",
      "LSTM inputs :  torch.Size([16, 16, 512])\n",
      "lstm_feat Output :  torch.Size([16, 16, 512])\n",
      "tcn_feat Output :  torch.Size([16, 16, 512])\n",
      "TLAB output shape :  torch.Size([16, 16, 512])\n",
      "audio_tlab feat :  torch.Size([16, 16, 512])\n",
      "LSTM inputs :  torch.Size([16, 16, 512])\n",
      "lstm_feat Output :  torch.Size([16, 16, 512])\n",
      "tcn_feat Output :  torch.Size([16, 16, 512])\n",
      "TLAB output shape :  torch.Size([16, 16, 512])\n",
      "video_tlab feat :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value1(video) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value2(audio) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn joint representation :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn X_v^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn X_a^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn value1 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn value2 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn weighted1 :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn weighted2 :  torch.Size([16, 16, 1024])\n",
      "DCNLayer X_att,v :  torch.Size([16, 16, 512])\n",
      "DCNLayer X_att,a :  torch.Size([16, 16, 512])\n",
      "LSTM inputs :  torch.Size([16, 16, 1024])\n",
      "audiovisual_vouts :  torch.Size([16, 16])\n",
      "audiovisual_aouts :  torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4295 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy\n",
      "-0.018203811194385407\n",
      "0.09929763250821377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(0, total_epoch):\n",
    "\tepoch_tic = time.time()\n",
    "\tlogging.info(\"Epoch\")\n",
    "\tlogging.info(epoch)\n",
    "\n",
    "\t# train for one epoch\n",
    "\tTraining_vacc, Training_aacc, Training_loss = train(trainloader, model, criterion, optimizer, scheduler, epoch, lr, fusion_model, time_chk_path=time_chk_path)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
