{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 건드리지 않기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, dim, num_layers, dropout, residual_embeddings=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.rnn_dim = dim // 2\n",
    "        self.linear = nn.Linear(dim + embed_size, dim)\n",
    "        self.rnn = nn.LSTM(embed_size, self.rnn_dim, num_layers=num_layers, dropout=dropout,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        self.residual_embeddings = residual_embeddings\n",
    "        self.init_hidden = nn.Parameter(nn.init.xavier_uniform_(torch.empty(2 * 2 * num_layers, self.rnn_dim)))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch = inputs.size(0)\n",
    "        h0 = self.init_hidden[:2 * self.num_layers].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "        c0 = self.init_hidden[2 * self.num_layers:].unsqueeze(1).expand(2 * self.num_layers,\n",
    "                                                                        batch, self.rnn_dim).contiguous()\n",
    "\n",
    "        print(\"LSTM inputs : \", inputs.shape)\n",
    "        outputs, hidden_t = self.rnn(inputs, (h0, c0))\n",
    "\n",
    "        if self.residual_embeddings:\n",
    "            outputs = torch.cat([inputs, outputs], dim=-1)\n",
    "        outputs = self.linear(self.dropout(outputs))\n",
    "\n",
    "        return F.normalize(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "class DenseCoAttn(nn.Module):\n",
    "\tdef __init__(self, dim1, dim2, dropout): #dim1, dim2 = 512, 512\n",
    "\t\tsuper(DenseCoAttn, self).__init__()\n",
    "\t\tdim = dim1 + dim2\n",
    "\t\tself.dropouts = nn.ModuleList([nn.Dropout(p=dropout) for _ in range(2)])\n",
    "\t\tself.query_linear = nn.Linear(dim, dim)\n",
    "\t\tself.key1_linear = nn.Linear(16, 16)\n",
    "\t\tself.key2_linear = nn.Linear(16, 16)\n",
    "\t\tself.value1_linear = nn.Linear(dim1, dim1)\n",
    "\t\tself.value2_linear = nn.Linear(dim2, dim2)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, value1, value2):\n",
    "\t\tprint(\"DenseCoAttn input value1(video) : \", value1.shape) # 16, 16, 512\n",
    "\t\tprint(\"DenseCoAttn input value2(audio) : \", value2.shape)\n",
    "\t\tjoint = torch.cat((value1, value2), dim=-1)\n",
    "\t\t# audio  audio*W*joint\n",
    "\t\tjoint = self.query_linear(joint)\n",
    "\t\tprint(\"DenseCoAttn joint representation : \", joint.shape)\n",
    "\t\tkey1 = self.key1_linear(value1.transpose(1, 2)) # X_v^T\n",
    "\t\tkey2 = self.key2_linear(value2.transpose(1, 2)) # X_a^T \n",
    "\t\tprint(\"DenseCoAttn X_v^T : \", key1.shape) # 16, 512, 16\n",
    "\t\tprint(\"DenseCoAttn X_a^T : \", key2.shape)\n",
    "\n",
    "\t\tvalue1 = self.value1_linear(value1) # 16, 16, 512 (Can't understanding Layer)\n",
    "\t\tvalue2 = self.value2_linear(value2) # (Can't understanding Layer)\n",
    "\t\tprint(\"DenseCoAttn value1 after value_linear : \", value1.shape)\n",
    "\t\tprint(\"DenseCoAttn value2 after value_linear : \", value2.shape)\n",
    "\n",
    "\t\tweighted1, attn1 = self.qkv_attention(joint, key1, value1, dropout=self.dropouts[0])\n",
    "\t\tweighted2, attn2 = self.qkv_attention(joint, key2, value2, dropout=self.dropouts[1])\n",
    "\t\tprint(\"DenseCoAttn weighted1 : \", weighted1.shape)\n",
    "\t\tprint(\"DenseCoAttn weighted2 : \", weighted2.shape)\n",
    "\n",
    "\t\treturn weighted1, weighted2\n",
    "\n",
    "\tdef qkv_attention(self, query, key, value, dropout=None):\n",
    "\t\td_k = query.size(-1)\n",
    "\t\tscores = torch.bmm(key, query) / math.sqrt(d_k)\n",
    "\t\tscores = torch.tanh(scores) # C_v, C_a\n",
    "\t\tif dropout:\n",
    "\t\t\tscores = dropout(scores)\n",
    "\n",
    "\t\tweighted = torch.tanh(torch.bmm(value, scores))\n",
    "\t\treturn self.relu(weighted), scores # self.relu(weighted) == H_v, H_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout): # dim1, dim2 = 512, 512\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.dense_coattn = DenseCoAttn(dim1, dim2, dropout)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim1), # 1024, 512\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        weighted1, weighted2 = self.dense_coattn(data1, data2) # weighted1, weighted2 = 1024, 1024\n",
    "        data1 = data1 + self.linears[0](weighted1) # X_att,v\n",
    "        data2 = data2 + self.linears[1](weighted2) # X_att,a\n",
    "\n",
    "        print(\"DCNLayer X_att,v : \" , data1.shape)\n",
    "        print(\"DCNLayer X_att,a : \" , data2.shape)\n",
    "\n",
    "        return data1, data2\n",
    "\n",
    "\n",
    "class DCNLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, num_seq, dropout): # dim1, dim2 = 512, 512\n",
    "        super(DCNLayer, self).__init__()\n",
    "        self.dcn_layers = nn.ModuleList([NormalSubLayer(dim1, dim2, dropout) for _ in range(num_seq)])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        for dense_coattn in self.dcn_layers:\n",
    "            data1, data2 = dense_coattn(data1, data2)\n",
    "\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "class BottomUpExtract(nn.Module):\n",
    "\tdef __init__(self, emed_dim, dim):\n",
    "\t\tsuper(BottomUpExtract, self).__init__()\n",
    "\t\tself.attn = PositionAttn(emed_dim, dim)\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tfeat = self.attn(video, audio)\n",
    "\n",
    "\t\treturn feat\n",
    "\n",
    "# audio-guided attention\n",
    "class PositionAttn(nn.Module):\n",
    "\n",
    "\tdef __init__(self, embed_dim, dim):\n",
    "\t\tsuper(PositionAttn, self).__init__()\n",
    "\t\tself.affine_audio = nn.Linear(embed_dim, dim)\n",
    "\t\tself.affine_video = nn.Linear(512, dim)\n",
    "\t\tself.affine_v = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_g = nn.Linear(dim, 49, bias=False)\n",
    "\t\tself.affine_h = nn.Linear(49, 1, bias=False)\n",
    "\t\tself.affine_feat = nn.Linear(512, dim)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, video, audio):\n",
    "\t\tv_t = video.view(video.size(0) * video.size(1), -1, 512).contiguous()\n",
    "\t\tV = v_t\n",
    "\n",
    "\t\t# Audio-guided visual attention\n",
    "\t\tv_t = self.relu(self.affine_video(v_t))\n",
    "\t\ta_t = audio.view(-1, audio.size(-1))\n",
    "\n",
    "\t\ta_t = self.relu(self.affine_audio(a_t))\n",
    "\n",
    "\t\tcontent_v = self.affine_v(v_t) \\\n",
    "\t\t\t\t\t+ self.affine_g(a_t).unsqueeze(2)\n",
    "\n",
    "\t\tz_t = self.affine_h((torch.tanh(content_v))).squeeze(2)\n",
    "\n",
    "\t\talpha_t = F.softmax(z_t, dim=-1).view(z_t.size(0), -1, z_t.size(1))  # attention map\n",
    "\n",
    "\t\tc_t = torch.bmm(alpha_t, V).view(-1, 512)\n",
    "\t\tvideo_t = c_t.view(video.size(0), -1, 512)\n",
    "\n",
    "\t\tvideo_t = self.affine_feat(video_t)\n",
    "\n",
    "\t\treturn video_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "\n",
    "class LSTM_CAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_CAM, self).__init__()\n",
    "        self.coattn = DCNLayer(512, 512, 2, 0.6)\n",
    "\n",
    "        self.audio_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True)\n",
    "        self.video_extract = LSTM(512, 512, 2, 0.1, residual_embeddings=True)\n",
    "\n",
    "        self.video_attn = BottomUpExtract(512, 512)\n",
    "        self.vregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        # self.Joint = LSTM(1024, 512, 2, dropout=0, residual_embeddings=True)\n",
    "        self.Joint = LSTM(2048, 512, 2, dropout=0, residual_embeddings=True)\n",
    "\n",
    "        self.aregressor = nn.Sequential(nn.Linear(512, 128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.6),\n",
    "                                 nn.Linear(128, 1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(net, init_type='xavier', init_gain=1):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "\n",
    "        def init_func(m):  # define the initialization function\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    init.uniform_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    init.xavier_uniform_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        print('initialize network with %s' % init_type)\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "    def forward(self, f1_norm, f2_norm):\n",
    "        video = F.normalize(f2_norm, dim=-1)\n",
    "        audio = F.normalize(f1_norm, dim=-1)\n",
    "\n",
    "        print(\"LSTM_CAM input video : \", video.shape)\n",
    "        print(\"LSTM_CAM input audio : \", audio.shape)\n",
    "        \n",
    "        # Tried with LSTMs also\n",
    "        audio = self.audio_extract(audio)\n",
    "        video = self.video_attn(video, audio)\n",
    "        video = self.video_extract(video)\n",
    "        print(\"LSTM_CAM after LSTM video : \", video.shape)\n",
    "        print(\"LSTM_CAM after LSTM audio : \", audio.shape)\n",
    "        \n",
    "        video, audio = self.coattn(video, audio)\n",
    "        print(\"LSTM_CAM after coattn video : \", video.shape)\n",
    "        print(\"LSTM_CAM after coattn audio : \", audio.shape)\n",
    "\n",
    "        video = F.pad(video, (0, 512))\n",
    "        audio = F.pad(audio, (0, 512))\n",
    "\n",
    "        print(\"LSTM_CAM after padding video : \", video.shape)\n",
    "        print(\"LSTM_CAM after padding audio : \", audio.shape)        \n",
    "        \n",
    "        audiovisualfeatures = torch.cat((video, audio), -1)\n",
    "        print(\"LSTM_CAM before padding audiovisualfeatures : \", audiovisualfeatures.shape)\n",
    "        # audiovisualfeatures = F.pad(audiovisualfeatures, (0, 1024))\n",
    "        \n",
    "        # print(\"LSTM_CAM after padding audiovisualfeatures : \", audiovisualfeatures.shape)\n",
    "        \n",
    "        audiovisualfeatures = self.Joint(audiovisualfeatures)\n",
    "        vouts = self.vregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "        aouts = self.aregressor(audiovisualfeatures) #.transpose(0,1))\n",
    "        print(\"LSTM_CAM after Joint audiovisualfeatures : \", audiovisualfeatures.shape)\n",
    "        print(\"LSTM_CAM vouts : \", vouts.shape)\n",
    "        print(\"LSTM_CAM aouts : \", aouts.shape)\n",
    "\n",
    "        print('--'*25)\n",
    "\n",
    "        return vouts.squeeze(2), aouts.squeeze(2)  #final_aud_feat.transpose(1,2), final_vis_feat.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tsav import TwoStreamAuralVisualModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'ABAW2020TNT/model2/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "saved_model = torch.load(model_path)\n",
    "model.load_state_dict(saved_model['state_dict'])\n",
    "\n",
    "new_first_layer = nn.Conv3d(in_channels=3,\n",
    "\t\t\t\t\tout_channels=model.video_model.r2plus1d.stem[0].out_channels,\n",
    "\t\t\t\t\tkernel_size=model.video_model.r2plus1d.stem[0].kernel_size,\n",
    "\t\t\t\t\tstride=model.video_model.r2plus1d.stem[0].stride,\n",
    "\t\t\t\t\tpadding=model.video_model.r2plus1d.stem[0].padding,\n",
    "\t\t\t\t\tbias=False)\n",
    "\n",
    "new_first_layer.weight.data = model.video_model.r2plus1d.stem[0].weight.data[:, 0:3]\n",
    "model.video_model.r2plus1d.stem[0] = new_first_layer\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with xavier\n",
      "LSTM_CAM input video :  torch.Size([16, 16, 25088])\n",
      "LSTM_CAM input audio :  torch.Size([16, 16, 512])\n",
      "LSTM inputs :  torch.Size([16, 16, 512])\n",
      "LSTM inputs :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM after LSTM video :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM after LSTM audio :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value1(video) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn input value2(audio) :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn joint representation :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn X_v^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn X_a^T :  torch.Size([16, 512, 16])\n",
      "DenseCoAttn value1 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn value2 after value_linear :  torch.Size([16, 16, 512])\n",
      "DenseCoAttn weighted1 :  torch.Size([16, 16, 1024])\n",
      "DenseCoAttn weighted2 :  torch.Size([16, 16, 1024])\n",
      "DCNLayer X_att,v :  torch.Size([16, 16, 512])\n",
      "DCNLayer X_att,a :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM after coattn video :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM after coattn audio :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM after padding video :  torch.Size([16, 16, 1024])\n",
      "LSTM_CAM after padding audio :  torch.Size([16, 16, 1024])\n",
      "LSTM_CAM before padding audiovisualfeatures :  torch.Size([16, 16, 2048])\n",
      "LSTM inputs :  torch.Size([16, 16, 2048])\n",
      "LSTM_CAM after Joint audiovisualfeatures :  torch.Size([16, 16, 512])\n",
      "LSTM_CAM vouts :  torch.Size([16, 16, 1])\n",
      "LSTM_CAM aouts :  torch.Size([16, 16, 1])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "audiodata = torch.empty((16, 16, 1, 64, 104)).cuda()\n",
    "visualdata = torch.empty((16, 16, 3, 8, 112, 112)).cuda()\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    with torch.no_grad():\n",
    "        visual_feats = torch.empty((16, 16, 25088), device = visualdata.device)\n",
    "        aud_feats = torch.empty((16, 16, 512), device = visualdata.device)\n",
    "\n",
    "        for i in range(16):\n",
    "            aud_feat, visualfeat, _ = model(audiodata[i,:,:,:], visualdata[i, :, :, :,:,:])\n",
    "            visual_feats[i,:,:] = visualfeat.view(16, -1)\n",
    "            aud_feats[i,:,:] = aud_feat\n",
    "\n",
    "cam = LSTM_CAM()\n",
    "result = cam(aud_feats, visual_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
